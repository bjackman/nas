# This is not how you're Supposed To Use Ansible - this should all be broken up into roles.
# But, it's fucking annoying to do things properly, instead here's one big dumb
# playbook file full of tasks.

- hosts: all
  vars:
    nas_base_dir: "{{ ansible_user_dir }}/nas"
    # This is where we'll store stuff on the NAS host that needs to persiste
    # across container restarts.
    nas_system_data_dir: "{{ nas_base_dir }}/system-data"
    # This is coupled with compose.yaml.
    filebrowser_database: "{{ nas_system_data_dir }}/filebrowser.db"
    sanoid_conf: |
      [{{ zfs_pool.name }}]
        use_template = production
        recursive = yes

      [template_production]
        frequently = 0
        hourly = 24
        daily = 30
        monthly = 11
        yearly = 1
        autoprune = yes
        prune_defer = 75
    # I got some OOMs which I think might be due to the ZFS ARC which can use a
    # lot of RAM. I asked ChatGPT if perhaps it the ARC doesn't integrate with
    # direct reclaim properly and it said yes, it made this claim pretty
    # convincingly, I'm inclined to believe it.
    # So, ensure that there's always 1.5G of RAM which ZFS won't eat.
    zfs_arc_max: "{{ ((ansible_memtotal_mb * 1024 * 1024) - (1.5 * 1024**3)) | int }}"

  tasks:
    # I dunno why installing the Ansible requirements is not just a default part
    # of the process of running a playbook, but for whatever reason it isn't.
    # I guess maybe they fucked up the dependency management somehow.
    # Anyway, whatever, just install it in the playbook. This is definitely not
    # Best Practice.
    - name: Install Ansible dependencies
      local_action: command ansible-galaxy install artis3n.tailscale geerlingguy.sanoid
      tags: ansible

    # If I just try to install the specific role it doesn't work, I dunno. So
    # install the whole collection.
    - name: Install Ansible dependencies for Prometheus
      local_action: command ansible-galaxy collection install prometheus.prometheus
      tags: ansible

    - name: Install NAS dependencies
      package:
        name:
          - zfsutils-linux
          - powertop
        state: present
      tags: base

    - name: Ensure NAS state directories exist
      file:
        path: "{{ item }}"
        state: directory
      with_items:
        - "{{ nas_base_dir }}"
        - "{{ nas_base_dir }}/prometheus"
        - "{{ nas_system_data_dir }}"
      tags: base

    # I don't really wanna use the auto-tune thing but I just can't be bothered
    # to figure out how to persist the specific settings.
    - name: Set up PowerTOP autotune service
      template:
        src: templates/powertop.service.jinja2
        dest: /etc/systemd/system/powertop.service
      become: true
      tags: powertop

    - name: Enable & start PowerTOP autotune service
      systemd:
        name: powertop
        daemon_reload: true
        enabled: true
        state: started
      become: true
      tags: powertop

    - name: Set up unprivileged HTTP server access
      sysctl:
        name: net.ipv4.ip_unprivileged_port_start
        value: 80
      become: true # sudo
      tags: base

    - name: Include Tailscale role if auth key is set
      include_role:
        name: artis3n.tailscale
      when: tailscale_authkey is defined
      tags: tailscale

    # Adapted from https://github.com/geerlingguy/arm-nas/blob/master/main.yml
    - name: Create ZFS pools.
      # zfs_pool will be read from host_vars. Each entry will be set as the
      # `item` in the command above.
      ansible.builtin.command:
        # Note - the 'members' contains the raidz1 bit.
        cmd: "zpool create -m {{ zfs_pool.mount_point }} {{ zfs_pool.name }} {{ zfs_pool.members }} -f"
        # Avoid trying to repeat the command above. There might be a tidier way
        # to do it than checking if the mountpoint exists, but whatever.
        creates: "{{ zfs_pool.mount_point }}"
      become: true # sudo
      tags: zfs

    - name: Ensure /etc/modprobe.d/zfs.conf exists
      copy:
        dest: /etc/modprobe.d/zfs.conf
        content: ""
        force: no  # Don't overwrite if it already exists
      become: true
      tags: zfs

    - name: Set zfs_arc_max in /etc/modprobe.d/zfs.conf
      lineinfile:
        path: /etc/modprobe.d/zfs.conf
        line: "options zfs zfs_arc_max={{ zfs_arc_max }}"
        regexp: "^options zfs zfs_arc_max="
        create: yes
      register: set_zfs_arch_max
      become: true
      tags: zfs

    - name: Update initramfs
      command: update-initramfs -u
      when: set_zfs_arch_max.changed
      become: true
      tags: zfs

    - name: Apply new ARC setting immediately
      command: "echo {{ zfs_arc_max }} > /sys/module/zfs/parameters/zfs_arc_max"
      become: true
      tags: zfs

    - name: Set up Sanoid (ZFS snapshots)
      include_role:
        name: geerlingguy.sanoid
        apply:
          # Give the tasks inside the role the zfs tag too.
          tags:
            - zfs
          become: true
          become_user: root
      tags: zfs

    - name: Create Docker system group
      group:
        name: docker
        state: present
        system: yes
      become: true
      tags: docker

    - name: Add current user to Docker group
      user:
        name: "{{ ansible_user_id }}"
        groups: docker
        append: yes
      become: true
      tags: docker

    - name: Install Docker snap
      snap:
        name: docker
      become: true
      tags: docker

    # Work around dumb docker-compose behaviour where it behaves differently
    # depending on whether a file exists on the host. If it doesn't, it will
    # mount it as a directory.
    - name: Ensure FileBrowser database exists
      copy:
        content: ""
        dest: "{{ filebrowser_database }}"
        force: false # Don't create if exists
      tags: filebrowser
      register: filebrowser_db

    # Create a FileBrowser config. Doing this explicitly is necessary because if
    # you just boot up FileBrowser with an empty config, the root creds are
    # admin:admin. FileBrowser has a JSON format for its user DB and its
    # configuration - it isn't documented but you can see it by doing dumbb
    # stuff like this:
    # docker run --rm -v ./system-data/filebrowser.db:/database.db -v ./fb-out:/fb-out filebrowser/filebrowser users export fb-out/users.json
    # docker run --rm -v ./system-data/filebrowser.db:/database.db -v ./fb-out:/fb-out filebrowser/filebrowser config export /fb-out/config.json
    # So we could have a config file in this repo and then import it. However,
    # that would be an annoying way to add users because we'd need to salt and
    # hash the password ourselves.
    # So, instead we just do this stupid imperative nonsense.
    - name: Initialise FileBrowser config
      docker_container:
        name: filebrowser-init
        image: filebrowser/filebrowser
        command: "config init"
        auto_remove: true
        volumes:
          - "{{ filebrowser_database }}:/database.db"
        detach: false
      tags: filebrowser
      # This will fail if the config is already initialised, so only do it if we
      # created one above.
      when: filebrowser_db.changed

    - name: Add FileBrowser user
      docker_container:
        name: filebrowser-user-add
        image: filebrowser/filebrowser
        command: "users add --perm.admin {{ filebrowser_username }} {{ filebrowser_password }}"
        auto_remove: true
        volumes:
          - "{{ filebrowser_database }}:/database.db"
        detach: false
      tags: filebrowser
      when: filebrowser_db.changed

    - name: Install inadyn config
      when: inadyn_config
      copy:
        content: "{{ inadyn_config }}"
        dest: "{{ nas_base_dir }}/inadyn.conf"
      tags: inadyn

    # Would like to do this via Docker but the rshared flag listed in the README
    # https://github.com/prometheus/node_exporter
    # doesn't work, because the rootfs is private.
    # So instead we just YOLO this shit, it installs it from a tarball from Github.
    - name: Install Prometheus node-exporter
      include_role:
        name: prometheus.prometheus.node_exporter
        apply:
          # Give the tasks inside the role the node-exporter tag too.
          tags:
            - node-exporter
      tags: node-exporter

    - name: Install Go SDK for ZFS Exporter
      apt:
        name: golang
      become: true
      tags: zfs-exporter

    - name: Install zfs-exporter binary
      command:
        cmd: go install github.com/pdf/zfs_exporter@latest
        creates: /usr/local/bin/zfs_exporter
      environment:
        GOBIN: /usr/local/bin
      become: true
      tags: zfs-exporter

    - name: Create ZFS Exporter service
      copy:
        src: files/zfs_exporter.service
        dest: /etc/systemd/system/zfs_exporter.service
      become: true
      tags: zfs-exporter

    - name: Enable ZFS Exporter service
      systemd_service:
        daemon_reload: true # Reload service files
        name: zfs_exporter
        state: restarted
        enabled: true # Start on boot
      become: true
      tags: zfs-exporter

    # Putting "when" and "with_items" at the top like this is also Not Best
    # Practice I think. But I like it.
    - when: smb_users
      with_items: "{{ smb_users }}"
      name: Create Samba user directories
      file:
        path: "{{ zfs_pool.mount_point }}/samba/{{ item.username }}"
        state: directory
        # Docker runs the containers as root, lmao. The janky Samba container
        # I'm using then creates an internal Unix user called smbuser ("force
        # user = smbuser" in smb.conf) which it tries to use for all filesystem
        # accesses. So the stupid workaround is just to let that user write the
        # directories by letting _all_ users write the directories.
        # It would not be any more stupid to just get the Samba containr to act
        # fully as root, but the container image I'm using doesn't provide an
        # obvious way to do that. To be honest it only seems worth fixing this
        # if migrating to a proper container runtime anyway.
        mode: "0777"
      tags: samba
      become: true

    - name: Push templated configs for stuff running in docker-compose
      template:
        src: "templates/{{ item }}.jinja2"
        dest: "{{ nas_base_dir }}/{{ item }}"
      tags:
        - docker-compose
        - templates
      with_items:
        - compose.yaml
        - Caddyfile
        - prometheus/prometheus.yaml
        - alertmanager.yaml

    - name: Push static configs for stuff running in docker-compose
      copy:
        src: "files/{{ item }}"
        dest: "{{ nas_base_dir }}/{{ item }}"
      tags: docker-compose
      with_items:
        - prometheus/rules.yaml

    - name: Start & restart docker-compose
      community.docker.docker_compose_v2:
        project_src: "{{ nas_base_dir }}"
        state: "{{ item }}"
      # docker-compose is a fucking pain in the arse, seems the only way to get
      # it to do the thing you obviously fucking want is to do both "up" _and_
      # "restart".
      with_items:
        - present # up
        - restarted
      tags: docker-compose
